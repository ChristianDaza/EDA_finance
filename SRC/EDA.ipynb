{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save data from database in AWS\n",
    "import db_utils as dbu\n",
    "# Get descritve informarion from the dataframe\n",
    "import extract_info as extract\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Creates plots for data\n",
    "import plot as plo\n",
    "# Tranform dataframe and columns\n",
    "import pre_processing as prep   \n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the RDSDatabaseConnector class from db_utils.py was used to connect to AWS RDS and download the data into the local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the AWS RDS credentials from yaml file\n",
    "db_credentials = dbu.read_credentials(\"/Users/ChAre/OneDrive/Desktop/aicore/EDA_finance/credentials.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise RDSDatabaseConnector class\n",
    "db_connector = dbu.RDSDatabaseConnector(db_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A connection was made, the data extracted and saved as a csv file into the local machine using save_data() method. This class method also called the methods for connecting and extracting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the AWS RDS, extract and save the data\n",
    "#db_connector.save_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the data was loaded into python as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into pyhton\n",
    "df_unclean = db_connector.load_data(\"./loan_payments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the dataframe colums were transformed to their appropriate data type, missing values were removed or imputed, the data was transformed to correct skeweness, outlier were removed and overly correlated columns were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54231 entries, 0 to 54230\n",
      "Data columns (total 43 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   id                           54231 non-null  int64  \n",
      " 1   member_id                    54231 non-null  int64  \n",
      " 2   loan_amount                  54231 non-null  int64  \n",
      " 3   funded_amount                51224 non-null  float64\n",
      " 4   funded_amount_inv            54231 non-null  float64\n",
      " 5   term                         49459 non-null  object \n",
      " 6   int_rate                     49062 non-null  float64\n",
      " 7   instalment                   54231 non-null  float64\n",
      " 8   grade                        54231 non-null  object \n",
      " 9   sub_grade                    54231 non-null  object \n",
      " 10  employment_length            52113 non-null  object \n",
      " 11  home_ownership               54231 non-null  object \n",
      " 12  annual_inc                   54231 non-null  float64\n",
      " 13  verification_status          54231 non-null  object \n",
      " 14  issue_date                   54231 non-null  object \n",
      " 15  loan_status                  54231 non-null  object \n",
      " 16  payment_plan                 54231 non-null  object \n",
      " 17  purpose                      54231 non-null  object \n",
      " 18  dti                          54231 non-null  float64\n",
      " 19  delinq_2yrs                  54231 non-null  int64  \n",
      " 20  earliest_credit_line         54231 non-null  object \n",
      " 21  inq_last_6mths               54231 non-null  int64  \n",
      " 22  mths_since_last_delinq       23229 non-null  float64\n",
      " 23  mths_since_last_record       6181 non-null   float64\n",
      " 24  open_accounts                54231 non-null  int64  \n",
      " 25  total_accounts               54231 non-null  int64  \n",
      " 26  out_prncp                    54231 non-null  float64\n",
      " 27  out_prncp_inv                54231 non-null  float64\n",
      " 28  total_payment                54231 non-null  float64\n",
      " 29  total_payment_inv            54231 non-null  float64\n",
      " 30  total_rec_prncp              54231 non-null  float64\n",
      " 31  total_rec_int                54231 non-null  float64\n",
      " 32  total_rec_late_fee           54231 non-null  float64\n",
      " 33  recoveries                   54231 non-null  float64\n",
      " 34  collection_recovery_fee      54231 non-null  float64\n",
      " 35  last_payment_date            54158 non-null  object \n",
      " 36  last_payment_amount          54231 non-null  float64\n",
      " 37  next_payment_date            21623 non-null  object \n",
      " 38  last_credit_pull_date        54224 non-null  object \n",
      " 39  collections_12_mths_ex_med   54180 non-null  float64\n",
      " 40  mths_since_last_major_derog  7499 non-null   float64\n",
      " 41  policy_code                  54231 non-null  int64  \n",
      " 42  application_type             54231 non-null  object \n",
      "dtypes: float64(20), int64(8), object(15)\n",
      "memory usage: 17.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Inspect the columns data types\n",
    "df_unclean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all columns were converted to their correct data type especially those who are categorical or dates type data. Therefore the columns were transformed using the methods in the DataTransform class in the pre_processing.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of preprocessing of DataTransform class for preprocessing\n",
    "df_prep = prep.DataTransform(df_unclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>funded_amount</th>\n",
       "      <th>funded_amount_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>instalment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_payment_date</th>\n",
       "      <th>last_payment_amount</th>\n",
       "      <th>next_payment_date</th>\n",
       "      <th>last_credit_pull_date</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>mths_since_last_major_derog</th>\n",
       "      <th>policy_code</th>\n",
       "      <th>application_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38676116</td>\n",
       "      <td>41461848</td>\n",
       "      <td>8000</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.49</td>\n",
       "      <td>248.82</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jan-2022</td>\n",
       "      <td>248.82</td>\n",
       "      <td>Feb-2022</td>\n",
       "      <td>Jan-2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38656203</td>\n",
       "      <td>41440010</td>\n",
       "      <td>13200</td>\n",
       "      <td>13200.0</td>\n",
       "      <td>13200.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.99</td>\n",
       "      <td>407.52</td>\n",
       "      <td>A</td>\n",
       "      <td>A3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jan-2022</td>\n",
       "      <td>407.52</td>\n",
       "      <td>Feb-2022</td>\n",
       "      <td>Jan-2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38656154</td>\n",
       "      <td>41439961</td>\n",
       "      <td>16000</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.49</td>\n",
       "      <td>497.63</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Oct-2021</td>\n",
       "      <td>12850.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Oct-2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38656128</td>\n",
       "      <td>41439934</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>14.31</td>\n",
       "      <td>514.93</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jun-2021</td>\n",
       "      <td>13899.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jun-2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38656121</td>\n",
       "      <td>41439927</td>\n",
       "      <td>15000</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>6.03</td>\n",
       "      <td>456.54</td>\n",
       "      <td>A</td>\n",
       "      <td>A1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jan-2022</td>\n",
       "      <td>456.54</td>\n",
       "      <td>Feb-2022</td>\n",
       "      <td>Jan-2022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54226</th>\n",
       "      <td>76597</td>\n",
       "      <td>76583</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>1775.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>9.01</td>\n",
       "      <td>159.03</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jul-2016</td>\n",
       "      <td>160.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jul-2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54227</th>\n",
       "      <td>142608</td>\n",
       "      <td>74724</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2350.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.59</td>\n",
       "      <td>162.73</td>\n",
       "      <td>C</td>\n",
       "      <td>C2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Oct-2016</td>\n",
       "      <td>490.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sep-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54228</th>\n",
       "      <td>117045</td>\n",
       "      <td>70978</td>\n",
       "      <td>3500</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>2225.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.43</td>\n",
       "      <td>108.77</td>\n",
       "      <td>A</td>\n",
       "      <td>A2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Sep-2016</td>\n",
       "      <td>110.58</td>\n",
       "      <td>Sep-2016</td>\n",
       "      <td>May-2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54229</th>\n",
       "      <td>88854</td>\n",
       "      <td>70699</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.43</td>\n",
       "      <td>155.38</td>\n",
       "      <td>A</td>\n",
       "      <td>A2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Mar-2014</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>May-2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54230</th>\n",
       "      <td>72323</td>\n",
       "      <td>70694</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.54</td>\n",
       "      <td>164.98</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>182.27</td>\n",
       "      <td>1.84</td>\n",
       "      <td>Mar-2014</td>\n",
       "      <td>164.97</td>\n",
       "      <td>Dec-2014</td>\n",
       "      <td>Jan-2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54231 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  member_id  loan_amount  funded_amount  funded_amount_inv  \\\n",
       "0      38676116   41461848         8000         8000.0             8000.0   \n",
       "1      38656203   41440010        13200        13200.0            13200.0   \n",
       "2      38656154   41439961        16000        16000.0            16000.0   \n",
       "3      38656128   41439934        15000        15000.0            15000.0   \n",
       "4      38656121   41439927        15000        15000.0            15000.0   \n",
       "...         ...        ...          ...            ...                ...   \n",
       "54226     76597      76583         5000         5000.0             1775.0   \n",
       "54227    142608      74724         5000         5000.0             2350.0   \n",
       "54228    117045      70978         3500         3500.0             2225.0   \n",
       "54229     88854      70699         5000         5000.0              225.0   \n",
       "54230     72323      70694         5000         5000.0              350.0   \n",
       "\n",
       "            term  int_rate  instalment grade sub_grade  ... recoveries  \\\n",
       "0      36 months      7.49      248.82     A        A4  ...       0.00   \n",
       "1      36 months      6.99      407.52     A        A3  ...       0.00   \n",
       "2      36 months      7.49      497.63     A        A4  ...       0.00   \n",
       "3      36 months     14.31      514.93     C        C4  ...       0.00   \n",
       "4      36 months      6.03      456.54     A        A1  ...       0.00   \n",
       "...          ...       ...         ...   ...       ...  ...        ...   \n",
       "54226  36 months      9.01      159.03     B        B2  ...       0.00   \n",
       "54227  36 months     10.59      162.73     C        C2  ...       0.00   \n",
       "54228  36 months      7.43      108.77     A        A2  ...       0.00   \n",
       "54229  36 months      7.43      155.38     A        A2  ...       0.00   \n",
       "54230  36 months     11.54      164.98     C        C5  ...     182.27   \n",
       "\n",
       "      collection_recovery_fee  last_payment_date last_payment_amount  \\\n",
       "0                        0.00           Jan-2022              248.82   \n",
       "1                        0.00           Jan-2022              407.52   \n",
       "2                        0.00           Oct-2021            12850.16   \n",
       "3                        0.00           Jun-2021            13899.67   \n",
       "4                        0.00           Jan-2022              456.54   \n",
       "...                       ...                ...                 ...   \n",
       "54226                    0.00           Jul-2016              160.61   \n",
       "54227                    0.00           Oct-2016              490.01   \n",
       "54228                    0.00           Sep-2016              110.58   \n",
       "54229                    0.00           Mar-2014                0.00   \n",
       "54230                    1.84           Mar-2014              164.97   \n",
       "\n",
       "      next_payment_date last_credit_pull_date collections_12_mths_ex_med  \\\n",
       "0              Feb-2022              Jan-2022                        0.0   \n",
       "1              Feb-2022              Jan-2022                        0.0   \n",
       "2                   NaN              Oct-2021                        0.0   \n",
       "3                   NaN              Jun-2021                        0.0   \n",
       "4              Feb-2022              Jan-2022                        0.0   \n",
       "...                 ...                   ...                        ...   \n",
       "54226               NaN              Jul-2016                        NaN   \n",
       "54227               NaN              Sep-2016                        0.0   \n",
       "54228          Sep-2016              May-2013                        NaN   \n",
       "54229               NaN              May-2013                        NaN   \n",
       "54230          Dec-2014              Jan-2015                        NaN   \n",
       "\n",
       "      mths_since_last_major_derog  policy_code  application_type  \n",
       "0                             5.0            1        INDIVIDUAL  \n",
       "1                             NaN            1        INDIVIDUAL  \n",
       "2                             NaN            1        INDIVIDUAL  \n",
       "3                             NaN            1        INDIVIDUAL  \n",
       "4                             NaN            1        INDIVIDUAL  \n",
       "...                           ...          ...               ...  \n",
       "54226                         NaN            1        INDIVIDUAL  \n",
       "54227                         NaN            1        INDIVIDUAL  \n",
       "54228                         NaN            1        INDIVIDUAL  \n",
       "54229                         NaN            1        INDIVIDUAL  \n",
       "54230                         NaN            1        INDIVIDUAL  \n",
       "\n",
       "[54231 rows x 43 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform columns into the categorical data type\n",
    "df_prep.category_transform([\"grade\", \"sub_grade\", \"home_ownership\", \"verification_status\", \"loan_status\", \"payment_plan\", \"purpose\", \"application_type\", \"employment_length\", \"policy_code\", \"term\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to date type data\n",
    "df_prep.date_transform([\"issue_date\", \"earliest_credit_line\", \"last_payment_date\", \"next_payment_date\", \"last_credit_pull_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the term column had has the \"month\" next to the actual value, this data would be more useful as numeric since this colums tell us the number of motnhly payment to the loan, which could  be use for calculations. However, first it was converted to string to remove \"month\" and then was converted to the integer type and the column name was updated to \"term_in_months\" to undertand the context of the numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column values into strings\n",
    "df_prep.string_transform([\"term\"])\n",
    "\n",
    "# Remove unwanted characters from the column values\n",
    "df_prep.remove_characters(\"term\", [\"months\"])\n",
    "\n",
    "# Transform column values to numeric\n",
    "df_prep.numeric_transform([\"term\"])\n",
    "\n",
    "#Rename column to reflect the unit for the values\n",
    "df_prep.rename_column(\"term\", \"term_in_months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing/imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrameInfo instance to extract useful information from the dataframe.\n",
    "df_inf = extract.DataFrameInfo(df_prep.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of the columns after the transformations.\n",
    "df_inf.check_columns_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of null per column of the dataframe was generated, to visualise the proportion on missing values across the whole data and to undertand which columns will need further pre_processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a count/percentage count of NULL values in each column\n",
    "df_inf.count_null(percentage=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base on the percentage of missing data I decided to:\n",
    "\n",
    "Drop:\n",
    " - mths_since_last_delinq:        57.2% missing values\n",
    " - mths_since_last_record:        88.6% missing values\n",
    " - mths_since_last_major_derog:   86.2% missing values\n",
    " \n",
    " Based on the large proprotion of missing data. Although these data could be imputed if needed it would most likely require machine learning and therefore an extended period of time. \n",
    "\n",
    "Impute:\n",
    "- funded_amount:                   5.5% missing values\n",
    "- term_in_months:                  8.8% missing values\n",
    "- int_rate:                        9.5% missing values\n",
    "- employment_length:               3.9% missing values\n",
    "- last_payment_date:               0.1% missing values\n",
    "- last_credit_pull_date:           0.013% missing values\n",
    "- collections_12_mths_ex_med:      0.094% missing values\n",
    "- next_payment_date:               86.2%  missing values\n",
    "\n",
    "Although the next_payment_date column has 86.2% of missing values, it may be useful to project the profit for the following month but I chose to impute it. I also prioritise imputing values rather than dropping rows since it  will reduce the data in other column decreasing the accuracy of imputation for other values  and inference made from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "df_prep.remove_columns([\"mths_since_last_delinq\", \"mths_since_last_record\", \"mths_since_last_major_derog\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normality test were carried on numerical and date type columns to undertand their distribution to select wether the mean, media or mode is the most appropriate method for imputation.Therefore, the Plotter class was used to visualise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate instance\n",
    "plots = plo.Plotter(df_prep.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funded amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality test\n",
    "df_inf.norm_test(\"funded_amount\")\n",
    "# Historgram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column=\"funded_amount\",\n",
    "                title= \"The total amount committed to the loan at that point in time\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "# QQplot\n",
    "plots.plot_qq(dataframe = df_inf.dataframe,\n",
    "              column = \"funded_amount\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the normality test had a significant P-value (p<0.05), both the histogram and QQ-plot showed that the data has a slight positive skew. Therefore the median will be used to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing Values\n",
    "\n",
    "# Get the mendian\n",
    "df_inf.descriptive_stats(selected_column=[\"funded_amount\"])\n",
    "\n",
    "# Replace missing values with the median\n",
    "df_prep.replace_null(\"funded_amount\", 12000.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term in months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality test\n",
    "df_inf.norm_test(\"term_in_months\")\n",
    "\n",
    "# Historgram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column=\"term_in_months\",\n",
    "                title= \"Distribution of the total monthly payments for the loan\",\n",
    "                xlabel=\"Total monthly payments\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# QQplot\n",
    "plots.plot_qq(dataframe= plots.dataframe,\n",
    "              column=\"term_in_months\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the histogram and QQ plot showed no normal distribution although the normality test's p value was significant. Also the histrogram could be interpreted as the data having positive skew. Therefore I will impute using the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values of the \"term_in_months\" column\n",
    "\n",
    "# Get the mendian\n",
    "df_inf.descriptive_stats(selected_column=[\"term_in_months\"])\n",
    "# replace null values\n",
    "df_prep.replace_null(\"term_in_months\", 36.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interest rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality test\n",
    "df_inf.norm_test(\"int_rate\")\n",
    "\n",
    "# Historgram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column= \"int_rate\",\n",
    "                title= \"Annual (APR) interest rate of the loan\",\n",
    "                xlabel=\"Annual inetrest rates\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "\n",
    "# QQplot\n",
    "plots.plot_qq(dataframe= plots.dataframe,\n",
    "              column=\"int_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the normality test showed a significant p value, both the histogram and QQ-plot displayed sligthly positive skewness of the data. Therefore i will use the median to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values \n",
    "\n",
    "# Get the mendian\n",
    "df_inf.descriptive_stats(selected_column=[\"int_rate\"])\n",
    "\n",
    "# replace null values\n",
    "df_prep.replace_null(\"int_rate\", 13.16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lats payment date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historgram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column= \"last_payment_date\",\n",
    "                title= \"Date on which last month payment was received\",\n",
    "                xlabel=\"Dates\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram showed prominent negative skew of the data but the QQplot and normality test could not be used to to conflict with the date type data. Thereofre, missing values will be imputed using the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values \n",
    "\n",
    "# Get the mendian\n",
    "df_inf.descriptive_stats( selected_column=[\"last_payment_date\"])\n",
    "\n",
    "# replace null values\n",
    "df_prep.replace_null(\"last_payment_date\",  pd.to_datetime('2021-04-01 00:00:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Last credit pull date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Historgram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column= \"last_credit_pull_date\",\n",
    "                title= \"Distribution of the most recent month that the loan company pulled credit for this loan\",\n",
    "                xlabel=\"Dates\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram showed prominent negative skewness of the data. Therefore the median will be used to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values\n",
    "\n",
    "# Get the mendian\n",
    "df_inf.descriptive_stats( selected_column=[\"last_credit_pull_date\"])\n",
    "\n",
    "# replace null values\n",
    "df_prep.replace_null(\"last_credit_pull_date\",  pd.to_datetime('2022-01-01 00:00:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Collections 12 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality test\n",
    "df_inf.norm_test(\"collections_12_mths_ex_med\")\n",
    "\n",
    "# Historgram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column=\"collections_12_mths_ex_med\",\n",
    "                title= \"Distribution of the Number of collections in 12 months excluding medical collections\",\n",
    "                xlabel=\"Number of collections\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# QQplot\n",
    "plots.plot_qq(dataframe= plots.dataframe,\n",
    "              column=\"collections_12_mths_ex_med\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the normality test p value is significant the  histogram and qq plot, showed a non normal distribution. However, this might be due to insufficient data but for this the data was treated as having a positive skewed and so the median was used to inpute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute values\n",
    "\n",
    "# Get the median\n",
    "df_inf.descriptive_stats(selected_column=[\"collections_12_mths_ex_med\"])\n",
    "# replace null values\n",
    "df_prep.replace_null(\"collections_12_mths_ex_med\", 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next payment date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histogram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column=\"next_payment_date\",\n",
    "                title= \"Distribution of the next scheduled payment date\",\n",
    "                xlabel='Next schedualed payment date',\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram showed negative skew of the data. Therefore missing values were imputed based on the median. The QQplot and normality test coudl not be done due to conflicts with the date data type. I acknowlwedge that since there is a large protion of data missing in this column imputing missing values based on median, mean or mode is not the best choice. However, the alternative would be using machine learning but the distance between null values an actual values in the column had to be taken into account since imputing them based their nearest neighbour would not be relaible in my opinion if there is a large gap between actual values and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mendian\n",
    "df_inf.descriptive_stats( selected_column=[\"next_payment_date\"])\n",
    "\n",
    "# replace null values\n",
    "df_prep.replace_null(\"next_payment_date\",  pd.to_datetime('2022-02-01 00:00:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Employment length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plots.hist_plot(dataframe= plots.dataframe,\n",
    "                column=\"employment_length\",\n",
    "                title= \"Duration in which the client has been working\",\n",
    "                xlabel=\"Years of employment\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram showed positive skew of the data. However, since the data is categorical the mode was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values\n",
    "# Get the mode\n",
    "df_prep.dataframe[\"employment_length\"].mode()\n",
    "# replace null values\n",
    "df_prep.replace_null(\"employment_length\", \"10+ years\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.missing_values_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe wiht the imputation will be save locally and be used for further sections with require transformation. This action was taken to prevent the transformations affecting the actual values of the data that will be used for the analysis sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_prep.dataframe.to_csv(\"impute_loan_payments\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the skewness of the numeric columns was measure, if their skewness was bigger than or equla to 0.5 they were considered skewed. Their skewness were corrected using a combination of histogram to visualise the skewness, transformations (log, Box-Cox, Yeo-Johnson) to alter the distribution shape and histogram to select the most effective transformation. In some cases the Box-Cox transformation was omitted due to the presence of zero and negative numbers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new intance with the impute dataframe to test for skewness\n",
    "df_inf_imp = extract.DataFrameInfo(df_prep.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "df_inf_imp.skew_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns were classified as skewed and therefore will be transformed.\n",
    "\n",
    "- loan_amount:        0.81 \n",
    "- funded_amount:      0.87\n",
    "- funded_amount_inv:  0.81 \n",
    "- term_in_moths:      1.15\n",
    "- instalmet:          1.0\n",
    "- annual_inc:         8.71  \n",
    "- delinq_2yrs:        5.37 \n",
    "- inq_last_6mths:     3.25\n",
    "- open_accounts:      1.06\n",
    "- total_accounts:     0.78\n",
    "- out_prncp:          2.36 \n",
    "- out_prncp_inv:      2.36\n",
    "- total_payment:      1.27 \n",
    "- total_payment_inv:  1.26 \n",
    "- total_rec_prncp:    1.26\n",
    "- total_rec_int:      2.2 \n",
    "- total_rec_late_fee: 13.18 \n",
    "- recoveries:         14.59 \n",
    "- collection_recovery_fee: 27.64\n",
    "- last_payment_amount: 2.5\n",
    "- collections_12_mths_ex_med: 20.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loan amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the skew\n",
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"loan_amount\",\n",
    "                title= \"Amount of loan the applicant received\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "loan_log_transform = df_prep.skew_transform(df_prep.dataframe[\"loan_amount\"], transformation=\"log\")\n",
    "loan_BC_transform = df_prep.skew_transform(df_prep.dataframe[\"loan_amount\"], transformation=\"BC\")\n",
    "loan_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"loan_amount\"], transformation=\"YJ\")\n",
    "\n",
    "# Histogram for log transform data\n",
    "plots.hist_plot(dataframe= loan_log_transform,\n",
    "                column=\"log\",\n",
    "                title=\"Distribution of log transformed loan amount\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "\n",
    "# Histogram for Box-Cox transform data\n",
    "plots.hist_plot(dataframe= loan_BC_transform,\n",
    "                column=\"Box-Cox\",\n",
    "                title=\"Distribution of Box-Cox transformed loan amount\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Histogram for Yeo-Johnson transform data\n",
    "plots.hist_plot(dataframe= loan_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title=\"Distribution of Yeo-Johnson transformed loan amount\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QQplot\n",
    "plots.plot_qq(dataframe= loan_log_transform,\n",
    "              column=\"log\")\n",
    "\n",
    "plots.plot_qq(dataframe= loan_BC_transform,\n",
    "              column=\"Box-Cox\")\n",
    "\n",
    "plots.plot_qq(dataframe= loan_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Box-Cox and Yeo-Johnson trasnformation produced the greater effect in the distribution.\n",
    "\n",
    "- Applied Box-Cox since there was not presence of negative or zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applied transformation\n",
    "df_prep.dataframe[\"loan_amount\"] = loan_log_transform[\"log\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funded amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"funded_amount\",\n",
    "                title= \"The total amount committed to the loan at that point in time\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Positive skew data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "funded_am_log_transform = df_prep.skew_transform(df_prep.dataframe[\"funded_amount\"], transformation=\"log\")\n",
    "funded_am_BC_transform = df_prep.skew_transform(df_prep.dataframe[\"funded_amount\"], transformation=\"BC\")\n",
    "funded_am_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"funded_amount\"], transformation=\"YJ\")\n",
    "\n",
    "# Histogram log\n",
    "plots.hist_plot(dataframe= funded_am_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \" Log transformed total amount committed to the loan at that point in time\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Histogram Box-Cox\n",
    "plots.hist_plot(dataframe= funded_am_BC_transform,\n",
    "                column=\"Box-Cox\",\n",
    "                title= \"Box-Cox transformed total amount committed to the loan at that point in time\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Histogram Yeo-Johnson\n",
    "plots.hist_plot(dataframe= funded_am_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \"Yeo-Johnson transformed total amount committed to the loan at that point in time\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=funded_am_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=funded_am_BC_transform,\n",
    "              column=\"Box-Cox\")\n",
    "plots.plot_qq(dataframe=funded_am_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Box-Cox and Yeo-Johnson produce the greates effect\n",
    "- Box-Cox was chosen since the data dosen't have any zero or nagative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skew transformation\n",
    "df_prep.dataframe[\"funded_amount\"] = funded_am_BC_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funded amount inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the skew of funded_amount_inv\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df_prep.dataframe, x=\"funded_amount_inv\", bins= 10)\n",
    "plt.title('Distribution of the total amount committed by the insvestors for that loan at that point in time')\n",
    "plt.xlabel('Total amount comitted in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive skew data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "funded_log_transform = df_prep.skew_transform(df_prep.dataframe[\"funded_amount_inv\"], transformation=\"log\")\n",
    "funded_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"funded_amount_inv\"], transformation=\"YJ\")\n",
    "\n",
    "# Histogram for log transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=funded_log_transform, x=\"log\", bins= 10)\n",
    "plt.title('Log transformed of the total amount committed by investors for that loan at that point in time')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\") \n",
    "plt.show()\n",
    "\n",
    "# Histogram for Yeo-Johnson transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=funded_YJ_transform, x=\"Yeo-Johnson\", bins= 10)\n",
    "plt.title('Yeo-Johnson transformed of the total amount committed by investors for that loan at that point in time')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQplot\n",
    "qq_plot = qqplot(funded_log_transform[\"log\"], scale=1 ,line='q')\n",
    "qq_plot = qqplot(funded_YJ_transform[\"Yeo-Johnson\"], scale=1 ,line='q')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yeo-Johnson trasnformation produce the greater effect and therefore was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"funded_amount_inv\"] = funded_YJ_transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instalment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of original data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df_prep.dataframe, x=\"instalment\", bins= 10)\n",
    "plt.title('Monthly payment owned by the borrower including interest')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Positive skew data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "instal_log_transform = df_prep.skew_transform(df_prep.dataframe[\"instalment\"], transformation=\"log\")\n",
    "instal_BC_transform = df_prep.skew_transform(df_prep.dataframe[\"instalment\"], transformation=\"BC\")\n",
    "instal_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"instalment\"], transformation=\"YJ\")\n",
    "\n",
    "# Histogram for log transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=instal_log_transform, x=\"log\", bins= 10)\n",
    "plt.title('Log transformed of the monthly payment owned by the borrower including interest')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\") \n",
    "plt.show()\n",
    "\n",
    "# Histogram for log transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=instal_BC_transform, x=\"Box-Cox\", bins= 10)\n",
    "plt.title('Cox-Box transformed of the monthly payment owned by the borrower including interest')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\") \n",
    "plt.show()\n",
    "\n",
    "# Histogram for Yeo-Johnson transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=instal_YJ_transform, x=\"Yeo-Johnson\", bins= 10)\n",
    "plt.title('Yeo-Johnson transformed of the monthly payment owned by the borrower including interest')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QQplot\n",
    "qq_plot = qqplot(instal_log_transform[\"log\"], scale=1 ,line='q')\n",
    "qq_plot = qqplot(instal_BC_transform[\"Box-Cox\"], scale=1 ,line='q')\n",
    "qq_plot = qqplot(instal_YJ_transform[\"Yeo-Johnson\"], scale=1 ,line='q')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Box-Cox and Yeo-Johnson produced the same results but since there were no zero or negative  values, Box-Cox was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"instalment\"]  = instal_BC_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Annual inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df_prep.dataframe, x=\"annual_inc\", bins= 5)\n",
    "plt.title('annual income of the borrower')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows left skew of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "an_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"annual_inc\"], transformation=\"log\")\n",
    "an_in_BC_transform = df_prep.skew_transform(df_prep.dataframe[\"annual_inc\"], transformation=\"BC\")\n",
    "an_in_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"annual_inc\"], transformation=\"YJ\")\n",
    "\n",
    "# Histogram for log transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=an_in_log_transform, x=\"log\", bins= 10, kde=True)\n",
    "plt.title('Log transformed of the annual income of the borrower')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\") \n",
    "plt.show()\n",
    "\n",
    "# Histogram for log transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=an_in_BC_transform, x=\"Box-Cox\", bins= 10, kde=True)\n",
    "plt.title('Cox-Box transformed of the annual income of the borrower')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\") \n",
    "plt.show()\n",
    "\n",
    "# Histogram for Yeo-Johnson transform data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=an_in_YJ_transform, x=\"Yeo-Johnson\", bins= 10, kde=True)\n",
    "plt.title('Yeo-Johnson transformed of the annual income of the borrower')\n",
    "plt.xlabel('Amount in pounds')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQplot\n",
    "qq_plot = qqplot(an_in_log_transform[\"log\"], scale=1 ,line='q')\n",
    "qq_plot = qqplot(an_in_BC_transform[\"Box-Cox\"], scale=1 ,line='q')\n",
    "qq_plot = qqplot(an_in_YJ_transform[\"Yeo-Johnson\"], scale=1 ,line='q')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms an qq plost showed that any of the three used transfomations can be use and produce very similar chnages in distribution but seems that the log transformation produces the most normal distribution. therefore this will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"annual_inc\"]  = an_in_log_transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"total_payment\", \n",
    "                title= \"Payments received to date for total amount funded\", \n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram shows positve/ right skew of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "total_p_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"total_payment\"], transformation=\"log\")\n",
    "total_p_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"total_payment\"], transformation=\"YJ\")\n",
    "\n",
    "# Plot log transformation\n",
    "plots.hist_plot(dataframe = total_p_in_log_transform,\n",
    "                column=\"log\", \n",
    "                title= \"Log transformed payments received to date for the total amount funded\", \n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Plot Box-Cox transformation\n",
    "plots.hist_plot(dataframe = total_p_YJ_transform,\n",
    "                column=\"Yeo-Johnson\", \n",
    "                title= \"Yeo-Johnson transformed Payments received to date for the total amount funded\", \n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=total_p_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=total_p_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms and qq plots showed that the Yeo-Johnson trasnformation ahs the greates effect on the data. Therefore we will applied that transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"total_payment\"] = total_p_YJ_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total rec int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original data\n",
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"total_rec_int\",\n",
    "                title= \"Interest received to date\",\n",
    "                xlabel=\"Amount of inetrest\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "total_rec_log_transform = df_prep.skew_transform(df_prep.dataframe[\"total_rec_int\"], transformation=\"log\")\n",
    "\n",
    "total_rec_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"total_rec_int\"], transformation=\"YJ\")\n",
    "\n",
    "# Histograms\n",
    "plots.hist_plot(dataframe= total_rec_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log transformed interest received to date\",\n",
    "                xlabel=\"Amount of inetrest\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "plots.hist_plot(dataframe= total_rec_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \"Yeo-Johnson transformed interest received to date\",\n",
    "                xlabel=\"Amount of inetrest\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QQ_plots\n",
    "plots.plot_qq(dataframe=total_rec_log_transform ,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=total_rec_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Yeo-Johnson transformation has the most effect on the data and therefore it will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"total_rec_int\"] = total_rec_YJ_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delinq 2yrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"delinq_2yrs\",\n",
    "                title= \"The number of 30+ days past-due payments in the borrower's credit file for the past 2 years\",\n",
    "                xlabel=\"Days\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram show postive/right skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delinq_log_transform = df_prep.skew_transform(df_prep.dataframe[\"delinq_2yrs\"], transformation=\"log\")\n",
    "delinq_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"delinq_2yrs\"], transformation=\"YJ\")\n",
    "\n",
    "plots.hist_plot(dataframe= delinq_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log transform number of 30+ days past-due payments in the borrower's credit file for the past 2 years\",\n",
    "                xlabel=\"Days\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "plots.hist_plot(dataframe= delinq_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \"Yeo-Johnson transform number of 30+ days past-due payments in the borrower's credit file for the past 2 years\",\n",
    "                xlabel=\"Days\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=delinq_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=delinq_YJ_transform ,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"delinq_2yrs\"]  = delinq_log_transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Out prncp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"out_prncp\",\n",
    "                title= \"Remaining outstanding principal for total amount funded\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows positive/right skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "out_pr_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"out_prncp\"], transformation=\"log\")\n",
    "out_pr_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"out_prncp\"], transformation=\"YJ\")\n",
    "\n",
    "# Plot log transformation\n",
    "plots.hist_plot(dataframe= out_pr_in_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log transformed remaining outstanding principal for total amount funded\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Plot Box-Cox Yeo-Johnson\n",
    "plots.hist_plot(dataframe = out_pr_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                bins_size= 10, \n",
    "                title= \"Yeo-Johnson transformed remaining outstanding principal for total amount funded\", \n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=out_pr_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=out_pr_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms and qq plots showed that the transformation do not have great affect in normalising the data. Since the log transformation is the closet to normal distribution between the I will proceed wiht this transformation. However, this data  will require another transformation that brings the data closer to a normal distirbution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"out_prncp\"] = out_pr_in_log_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"out_prncp_inv\",\n",
    "                title= \"Remaining outstanding principal for portion of total amount funded by investors\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram showed positve right skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "out_in_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"out_prncp_inv\"], transformation=\"log\")\n",
    "out_in_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"out_prncp_inv\"], transformation=\"YJ\")\n",
    "\n",
    "# Log transformation\n",
    "plots.hist_plot(dataframe= out_in_in_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Remaining outstanding principal for portion of total amount funded by investors\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Log transformation\n",
    "plots.hist_plot(dataframe= out_in_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \"Remaining outstanding principal for portion of total amount funded by investors\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=out_in_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=out_in_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qq plots showed that either transformation cannot fully correct the skew of the data. However, since the distribution of the log transform data is closer to a normal distribtuion than the Yeo-Johnson, this one will be use for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"out_prncp_inv\"] = out_in_in_log_transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recoveries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"recoveries\",\n",
    "                title= \"Post charge off gross recovery\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows, positive/ rigth skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "recover_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"recoveries\"], transformation=\"log\")\n",
    "recover_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"recoveries\"], transformation=\"YJ\")\n",
    "\n",
    "# Log transformation\n",
    "plots.hist_plot(dataframe= recover_in_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log transformed post charge off gross recovery\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Log transformation\n",
    "plots.hist_plot(dataframe= recover_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \" Yeo-Johnson transformed post charge off gross recovery\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=recover_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=recover_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qq plots showed that either transformation cannot fully correct the skew of the data. However, since the distribution of the log transform data is closer to a normal distribtuion than the Yeo-Johnson, this one will be use for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"recoveries\"] = recover_in_log_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collection recovery fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"collection_recovery_fee\", \n",
    "                title= \"Post charge off collection fee\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\",\n",
    "                bins_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "collection_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"collection_recovery_fee\"], transformation=\"log\")\n",
    "collection_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"collection_recovery_fee\"], transformation=\"YJ\")\n",
    "\n",
    "# Plot log transformation\n",
    "plots.hist_plot(dataframe= collection_in_log_transform,\n",
    "                column=\"log\", \n",
    "                title= \"Log transform post charge off collection fee\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\",\n",
    "                bins_size=5)\n",
    "\n",
    "# Plot Box-Cox transformation\n",
    "plots.hist_plot(dataframe = collection_YJ_transform,\n",
    "                 column=\"Yeo-Johnson\",\n",
    "                 title= \"Yeo-Johnson transformed post charge off collection fee\",\n",
    "                 xlabel=\"Amount in pounds\",\n",
    "                 ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=collection_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=collection_YJ_transform,\n",
    "               column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qqplots showed that the log transformation has the greates effect in correcting the skenes and therefore it will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"collection_recovery_fee\"] = collection_in_log_transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"total_rec_late_fee\",\n",
    "                title= \"Late fees received to date\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "total_p_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"total_rec_late_fee\"], transformation=\"log\")\n",
    "total_p_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"total_rec_late_fee\"], transformation=\"YJ\")\n",
    "\n",
    "plots.hist_plot(dataframe= total_p_in_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log transformed late fees received to date\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "plots.hist_plot(dataframe= total_p_YJ_transform,\n",
    "                 column=\"Yeo-Johnson\",\n",
    "                 title= \"Yeo-Johnson transformed late fees received to date\",\n",
    "                 xlabel=\"Amount in pounds\",\n",
    "                 ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=total_p_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=total_p_YJ_transform,\n",
    "               column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qqplots showed that the log transformation has the greates effect in correcting the skenes and therefore it will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"total_rec_late_fee\"] = total_p_in_log_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inq last 6mths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"inq_last_6mths\",\n",
    "                title= \"The number of inquiries in past 6 months (excluding auto and mortgage inquiries)\",\n",
    "                xlabel=\"Inquiries\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram showed positive/right skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "inq_last_log_transform = df_prep.skew_transform(df_prep.dataframe[\"inq_last_6mths\"], transformation=\"log\")\n",
    "inq_last_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"inq_last_6mths\"], transformation=\"YJ\")\n",
    "\n",
    "# Plot log transformation\n",
    "plots.hist_plot(dataframe = inq_last_log_transform ,\n",
    "                column=\"log\",\n",
    "                title= \"Log transformed number of inquiries in past 6 months (excluding auto and mortgage inquiries)\",\n",
    "                xlabel=\"inquiries\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "\n",
    "\n",
    "# Plot Box-Cox transformation\n",
    "plots.hist_plot(dataframe = inq_last_YJ_transform,\n",
    "                 column=\"Yeo-Johnson\",\n",
    "                 title= \"Yeo-Johnson transformed number of inquiries in past 6 months (excluding auto and mortgage inquiries)\",\n",
    "                 xlabel=\"inquiries\",\n",
    "                 ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=inq_last_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(inq_last_YJ_transform,\n",
    "               column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qqplots showed that the log transformation has the greates effect in correcting the skenes and therefore it will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"inq_last_6mths\"] = inq_last_log_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"open_accounts\",\n",
    "                title= \"The number of open credit lines in the borrower's credit file\",\n",
    "                xlabel=\"Open accounts\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "open_a_log_transform = df_prep.skew_transform(df_prep.dataframe[\"open_accounts\"], transformation=\"log\")\n",
    "open_a_BC_transform = df_prep.skew_transform(df_prep.dataframe[\"open_accounts\"], transformation=\"BC\")\n",
    "open_a_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"open_accounts\"], transformation=\"YJ\")\n",
    "\n",
    "# Log transformation\n",
    "plots.hist_plot(dataframe= open_a_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log transformed number of open credit lines in the borrower's credit file\",\n",
    "                xlabel=\"Open accounts\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "\n",
    "# Box-Cox transformation\n",
    "plots.hist_plot(dataframe=open_a_BC_transform,\n",
    "                column=\"Box-Cox\",\n",
    "                title= \"Box-Cox transformed number of open credit lines in the borrower's credit file\",\n",
    "                xlabel=\"Open accounts\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "# Plot Box-Cox transformation\n",
    "plots.hist_plot(dataframe = open_a_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \"Yeo-Johnson transformed number of open credit lines in the borrower's credit file\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=open_a_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=open_a_BC_transform,\n",
    "              column=\"Box-Cox\")\n",
    "plots.plot_qq(dataframe=open_a_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qqplots showed that the Box-Cox transformation had has the greates effect in bringign the distribution closer to normal. Although the the difference is not much greater than the effect of Yeo-Johnson transformation. Therefore the Box-Cox transformation will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"open_accounts\"] = open_a_BC_transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"total_accounts\",\n",
    "                title= \"The total number of credit lines currently in the borrower's credit file\",\n",
    "                xlabel=\"Number of accounts\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive/right skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ac_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"total_accounts\"], transformation=\"log\")\n",
    "total_ac_BC_transform = df_prep.skew_transform(df_prep.dataframe[\"total_accounts\"], transformation=\"BC\")\n",
    "total_ac_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"total_accounts\"], transformation=\"YJ\")\n",
    "\n",
    "plots.hist_plot(dataframe= total_ac_in_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log transformed total number of credit lines currently in the borrower's credit file\",\n",
    "                xlabel=\"Number of accounts\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "plots.hist_plot(dataframe= total_ac_BC_transform,\n",
    "                column=\"Box-Cox\",\n",
    "                title= \"Box-Cox transformed total number of credit lines currently in the borrower's credit file\",\n",
    "                xlabel=\"Number of accounts\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "plots.hist_plot(dataframe= total_ac_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \"Yeo-Johnson transformed total number of credit lines currently in the borrower's credit file\",\n",
    "                xlabel=\"Number of accounts\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=total_ac_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=total_ac_BC_transform,\n",
    "              column=\"Box-Cox\")\n",
    "plots.plot_qq(dataframe=total_ac_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qq plots shows that the box-Cox and Yeo-Johnson have the greates effect on the data. Since the data originally did not have any zero or negative values, the Box-Cox transformation will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"total_accounts\"] = total_ac_BC_transform "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Last payment amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"last_payment_amount\",\n",
    "                title= \"Last total payment amount received\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows positive/right skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations\n",
    "last_p_in_log_transform = df_prep.skew_transform(df_prep.dataframe[\"last_payment_amount\"], transformation=\"log\")\n",
    "last_p_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"last_payment_amount\"], transformation=\"YJ\")\n",
    "\n",
    "\n",
    "plots.hist_plot(dataframe= last_p_in_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \"Log trasnformed last total payment amount received\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "plots.hist_plot(dataframe= last_p_YJ_transform,\n",
    "                column=\"Yeo-Johnson\",\n",
    "                title= \"Yeo-Johnson trasnformed last total payment amount received\",\n",
    "                xlabel=\"Amount in pounds\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=last_p_in_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=last_p_YJ_transform,\n",
    "              column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qq plots show that the Yeo-Johnson transformation ahs the greates effect. Therefore it will be applied to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"last_payment_amount\"] = last_p_YJ_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collections 12 mths ex med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.hist_plot(dataframe= df_prep.dataframe,\n",
    "                column=\"collections_12_mths_ex_med\",\n",
    "                title= \"Number of collections in 12 months' excluding medical collections\",\n",
    "                xlabel=\"Collections\",\n",
    "                ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation.\n",
    "collections_log_transform = df_prep.skew_transform(df_prep.dataframe[\"collections_12_mths_ex_med\"], transformation=\"log\")\n",
    "collections_YJ_transform = df_prep.skew_transform(df_prep.dataframe[\"collections_12_mths_ex_med\"], transformation=\"YJ\")\n",
    "\n",
    "plots.hist_plot(dataframe= collections_log_transform,\n",
    "                column=\"log\",\n",
    "                title= \" Log transformed number of collections in 12 months' excluding medical collections\",\n",
    "                xlabel=\"Collections\",\n",
    "                ylabel=\"Frequency\")\n",
    "\n",
    "plots.hist_plot(dataframe= collections_YJ_transform,\n",
    "                 column=\"Yeo-Johnson\",\n",
    "                 title= \"Log transformed number of collections in 12 months' excluding medical collections\",\n",
    "                 xlabel=\"Collections\",\n",
    "                 ylabel=\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_qq(dataframe=collections_log_transform,\n",
    "              column=\"log\")\n",
    "plots.plot_qq(dataframe=collections_YJ_transform,\n",
    "               column=\"Yeo-Johnson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms and qq plots show that the log transformation has the greates effect. Therefore it will be use to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.dataframe[\"collections_12_mths_ex_med\"] = collections_log_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sections numeric type columsn were visulised or tested for outliers wich then they were either removed or left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"loan_amount\", \n",
    "               title=\"Number of 30+ days past-due payments in the borrower's credit file for the past 2 years\",\n",
    "               xlabel=\"Number of 30+ days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows there is an otulier at the left wisker of the boxplot. However, since is close to the wisker it dosen't seem likely that it is an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z scores, the cutts of to be consider and outlier is normally >=  2 or 3\n",
    "df_z = df_inf.z_score(dataframe=df_prep.dataframe, column=\"loan_amount\")\n",
    "# Turn on filter based on z_scores\n",
    "df_z_cutoff = df_inf.z_score(dataframe=df_prep.dataframe, column=\"loan_amount\", filter=True)\n",
    "df_z_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The z_scores showed that 23875 are considered and outlier, being z_score > 2. However, removing that many data points will affect all the distribution distribution for other columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idetify outlier based on IQR\n",
    "df_inf.IQR_filter_outliers(column=\"loan_amount\", dataframe=df_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interquartile range showed six data points with the same value, which means that the single outlier in the boxplot is actually six overlapping data points. Since all six outliers share the same value and are very close to the left wisker, they don't seem to be an error and therefore, they will remian in the column. The box-plot and interquantile range seem to less stringet than z_scores. Therefore both of them will be use for the subsequent columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"funded_amount\", \n",
    "               title=\"Total amount committed to the loan at that point in time\",\n",
    "               xlabel=\"Amount in pounds\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"funded_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows there are 2 otuliers at the left wisker of the boxplot, which slightly overlap with each other.However, the interquartile range shows that there are 6 outliers, of which five of them have the same values. Since 5 of the outliers share the same value and overlap wiht the 6th outlier all of them close to the left wisker of the boxplot, they don't seem to be an error. Therefore, they will remian in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"funded_amount_inv\", \n",
    "               title=\"The total amount committed by investors for that loan at that point in time\",\n",
    "               xlabel=\"Amount commited by investors in pounds\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"funded_amount_inv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and interquartile range showed 155 outliers near by the left wisker of the box plot. These most of which overlap with each other, except those wiht a zero value. Altough they are outisde of the wiskers, their clustering and overlapping suggest that these data points are no an error. Interially the data points wiht a zero value can be deleted but these can referring to loans funded by other means rather than investors. Taking into account that the outliers don't seem to be extreamily large, they will remain in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"int_rate\", \n",
    "               title=\"Annual interest rate of the loan\",\n",
    "               xlabel=\"Interest rates\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"int_rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and interquartile range showed 919 outliers near by the right wisker of the box plot. These most of which overlap with each other. Altough they are outisde of the wiskers, their clustering and overlapping suggest that these data points are no an error. To determine if these are left or remove the interest rate for their corresponding need to be check with the data for those years. Hoever, since their values son't seem to be very large they will remain in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"instalment\", \n",
    "               title=\"The monthly payment owned by the borrower, inclusive of the interest.\",\n",
    "               xlabel=\"Number of monthly paymets\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"instalment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and interquartile range showed 41 outliers near by the right wisker of the box plot. These most of which overlap with each other. Altough they are outisde of the wiskers, their clustering and overlapping suggest that these data points are no an error. To determine if these are left or remove the interest rate for their corresponding need to be check with the data for those years. Hoever, since their values son't seem to be very large they will remain in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"annual_inc\", \n",
    "               title=\"The annual income of the borrower.\",\n",
    "               xlabel=\"Annual income.\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_an_in = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"annual_inc\")\n",
    "df_filter_outliers_an_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and IQR_filter_outliers method shows 1335 ouliers. Altough there is a lot of overlapping and these data points don't seem to be an error, some of them have little over lapping. Therefore, we will remove all valu below 1.90, which are the values that less overlap between each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_an_in = df_filter_outliers_an_in[df_filter_outliers_an_in[\"annual_inc\"] < 1.90].index \n",
    "df_prep.remove_outlier(outlier_delete_index_an_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"annual_inc\", \n",
    "               title=\"Outliers removed annual income of the borrower.\",\n",
    "               xlabel=\"Annual income.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter_outliers_is_da = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"issue_date\")\n",
    "df_filter_outliers_is_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IQR_filter_outliers shows 23 outliers. However, since the issue dat for loan can depend in several factor, these data points will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"dti\", \n",
    "               title=\"DTI ratio\",\n",
    "               xlabel=\"Ratio.\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_dti = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"dti\")\n",
    "df_filter_outliers_dti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method shows 60 outliers close by the right wisker of the boxplot. Since these data points are overlapping and close to the wiskers, it suggest they are not an error and therefore  they will remain in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_ear_credit = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"earliest_credit_line\")\n",
    "df_filter_outliers_ear_credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QR_filter_outliers showed 1833 outliers. However, since dates can be affected by many  other variables, therefore these they will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_pre_im.dataframe, \n",
    "               column=\"open_accounts\", \n",
    "               title=\"The number of open credit lines in the borrower's credit file\",\n",
    "               xlabel=\"number of open credit lines\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_op = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"open_accounts\")\n",
    "df_filter_outliers_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method show 693 outliers, 2 data point clusters the left wisker and 21 data clusters in the right wisker. The outliers on the left wisker will be remove and any putlier above 7.2 in the right wisker, since the remaining data point are very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_op = df_filter_outliers_op[(df_filter_outliers_op[\"open_accounts\"] < 1) | (df_filter_outliers_op[\"open_accounts\"] > 7.2)].index \n",
    "df_prep.remove_outlier(outlier_delete_index_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"open_accounts\", \n",
    "               title=\"The number of open credit lines in the borrower's credit file\",\n",
    "               xlabel=\"number of open credit lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_accounts\", \n",
    "               title=\"total number of credit lines currently in the borrower's credit file\",\n",
    "               xlabel=\"Number of credit lines.\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_t_a = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"total_accounts\")\n",
    "df_filter_outliers_t_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method show 132 outliers, 1 data point clusters the left wisker and several others in the right wisker. The outliers on the left wisker will be remove and any putlier above 12.2 in the right wisker, since the the outliers begin to separate previous overlapping outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_t_a = df_filter_outliers_t_a[(df_filter_outliers_t_a[\"total_accounts\"] < 2.3) | (df_filter_outliers_t_a[\"total_accounts\"] > 12.2)].index \n",
    "df_prep.remove_outlier(outlier_delete_index_t_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep, \n",
    "               column=\"total_accounts\", \n",
    "               title=\"total number of credit lines currently in the borrower's credit file\",\n",
    "               xlabel=\"Number of credit lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_payment\", \n",
    "               title=\"Payments received to date for total amount funded\",\n",
    "               xlabel=\"Payments recieved.\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_t_p = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"total_payment\")\n",
    "df_filter_outliers_t_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method show 192 outliers with 3 clusters on the left wisker and two clustes in the right wisker. For both wisker  the outliers that are separating or in their own cluster away from the clusters touching the wisker will be be remove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_t_p = df_filter_outliers_t_p[(df_filter_outliers_t_p[\"total_payment\"] < 15) | (df_filter_outliers_t_p[\"total_payment\"] > 97)].index \n",
    "df_prep.remove_outlier(outlier_delete_index_t_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_payment\", \n",
    "               title=\"Payments received to date for total amount funded\",\n",
    "               xlabel=\"Payments recieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_payment_inv\", \n",
    "               title=\"Payments received to date for total amount funded by investores\",\n",
    "               xlabel=\"Payments recieved.\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_t_i = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"total_payment_inv\")\n",
    "df_filter_outliers_t_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method show 1711 outliers with 1 main clustes in the right wisker. Outlier above 47000 will be remove, which are the outliers that are separting from the main cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_t_i = df_filter_outliers_t_i[df_filter_outliers_t_i[\"total_payment_inv\"] > 47000].index \n",
    "df_prep.remove_outlier(outlier_delete_index_t_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_payment_inv\", \n",
    "               title=\"Payments received to date for total amount funded by investores\",\n",
    "               xlabel=\"Payments recieved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_rec_prncp\", \n",
    "               title=\"Total recieved principal for total amount funded\",\n",
    "               xlabel=\"Amount recive\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_t_rec_pr = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"total_rec_prncp\")\n",
    "df_filter_outliers_t_rec_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method show 2249 outliers with 1 main clustes in the right wisker. Outlier above 33000 will be remove, which are the outliers that are separting from the main cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_t_rec_pr = df_filter_outliers_t_rec_pr[df_filter_outliers_t_rec_pr[\"total_rec_prncp\"] > 30200].index \n",
    "df_prep.remove_outlier(outlier_delete_index_t_rec_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_rec_prncp\", \n",
    "               title=\"Total recieved principal for total amount funded\",\n",
    "               xlabel=\"Amount recive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_rec_int\", \n",
    "               title=\"Interest received to date\",\n",
    "               xlabel=\"Interest\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_t_r_i = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"total_rec_int\")\n",
    "df_filter_outliers_t_r_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method show 2249 outliers with 2 main clustes in the right wisker and 1 main cluster in the right wisker. Outlier less than 4 and above 28 will be remove, which are the outliers that are separting from the main cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_t_r_i = df_filter_outliers_t_r_i[(df_filter_outliers_t_r_i[\"total_rec_int\"] < 4) | (df_filter_outliers_t_r_i[\"total_rec_int\"] > 28) ].index \n",
    "df_prep.remove_outlier(outlier_delete_index_t_r_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"total_rec_int\", \n",
    "               title=\"Interest received to date\",\n",
    "               xlabel=\"Interest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_l_p_d= df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"last_payment_date\")\n",
    "df_filter_outliers_l_p_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QR_filter_outliers method show 1069 outliers. However since dates can be affected by several other variables, the data will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"last_payment_amount\", \n",
    "               title=\"Last total payment amount received\",\n",
    "               xlabel=\"Payment amount\")\n",
    "\n",
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_l_p_a = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"last_payment_amount\")\n",
    "df_filter_outliers_l_p_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boxplot and QR_filter_outliers method show 223 outliers with twom main cluster data points. Outliers less than 1.7 since from that values data points begin to separate from the main cluster touchin the wisker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_delete_index_l_p_a = df_filter_outliers_l_p_a[df_filter_outliers_l_p_a[\"last_payment_amount\"] < 1.7].index \n",
    "df_prep.remove_outlier(outlier_delete_index_l_p_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.box_plot(dataframe=df_prep.dataframe, \n",
    "               column=\"last_payment_amount\", \n",
    "               title=\"Last total payment amount received\",\n",
    "               xlabel=\"Payment amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_n_p_d = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"next_payment_date\")\n",
    "df_filter_outliers_n_p_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since dates can be infleunce by several other factors, this data will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idetify outlier based on IQR\n",
    "df_filter_outliers_l_c_p = df_inf.IQR_filter_outliers(dataframe=df_prep.dataframe, column=\"last_credit_pull_date\")\n",
    "df_filter_outliers_l_c_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since dates can be infleunce by several other factors, this data will remain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highly correlated column can predict with very high degree of accuracy, and thus making the dataframe more compact. Thios section aim tocheck  the correlation between the columns of the data frame and then select wether to remove or leave tose columns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix will be compute for the whole data frame to idetify possible correlating columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_prep.dataframe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure correlation between varibales of the dataframe can be seen predominately positive correlation across the dataset, such as:\n",
    "\n",
    "\n",
    "funded amount:\n",
    "- total_account\n",
    "- out_prncp \n",
    "- total_rec_late_fee linear\n",
    "- recoveries \n",
    "- collection_fee_linear\n",
    "\n",
    "fundedamount_inv:\n",
    "- out_prncp \n",
    "- out_prncp_inv\n",
    "\n",
    "\n",
    "instalmest:\n",
    "- loan_amount\n",
    "- annual_inc \n",
    "- out_prcncp \n",
    "- out_prcncp_inv \n",
    "- total_payment \n",
    "- total_payment_inv \n",
    "- total_rec_prcnp \n",
    "- total_rec_int \n",
    "\t\n",
    "annual_inc\n",
    "- loan_amount \n",
    "- instalment \n",
    "- ut_prcncp \n",
    "- out_prcncp_inv \n",
    "- total_payment_inv \n",
    "- total_rec_prcnp \n",
    "- total_rec_int "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualise the correlation between the variables in the dataset a heat map was created using pearson's correalation with a threshold value of 0.90. This satatictic was chosen because measures linear relationships and is sensitive to both magnitud and direction of the linear association. In addtion the data has been normalize and the outliers were dealt with. However, since all the outliers were deleted Kendall and Spearman could also be used since they are more robust and make fewer assumptions about the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "corr = df_prep.dataframe.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "# Generate  a palette\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Generate heatmap\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure show pearson's correalation predominately positive correlation in the dataset, particularly with:\n",
    "- loan_amount\n",
    "- funded_amount\n",
    "- funded_amount_inv\n",
    "- instalment\n",
    "- annual_inc\n",
    "- out_prncp\n",
    "- out_prncp\n",
    "- total_payment\n",
    "- total_payment_inv\n",
    "- total_rec_prncp\n",
    "- total_rec_int\n",
    "- total_rec_late_fee\n",
    "- recoveries\n",
    "- Collection_recovery_fee\n",
    "\n",
    "Interestintly the heatmap showed no correlation between recovery, collection_recovery_fee and out_prncp and out_prncp_inv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the threhold value of 0.90 these are the columns correlated:\n",
    "funded_amount: loan_amount\n",
    "\n",
    "funded_amount_inv: loan_amount, funded_amount\n",
    "\n",
    "Instalment: loan_amount\n",
    "\n",
    "annual_inc: loan_amount, instalment, funded_amount_inv\n",
    "\n",
    "total_payment_inv: total_payment\n",
    "\n",
    "total_rec_prncp: total_payment, total_payment_inv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regards to funded_amount, loan_amount, funded_amount_inv, instalmets and annual_inc. They reflect different importan aspects of the loan. Deleting them will hinder the interpredation adn further analysis of the data. Therefore these columns will remian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since total_rec_prncp is correlated to total_payment, total_payment_inv, which are more important for inepretation, total_rec_prncp could be removed. Therefore,  two linear regresion models will be created and based on the Variance Inflation Factor (IVF) values one of the columns will be removed. VIF is greater than 1 shows the presence of multicollinearity and therefore one of the columns will be remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model to try and predict total_rec_prncp \n",
    "t_r_p_model = smf.ols(\"total_rec_prncp ~ total_payment + total_payment_inv\", df_prep.dataframe).fit()\n",
    "t_p_model = smf.ols(\"total_payment ~ total_rec_prncp + total_payment_inv\", df_prep.dataframe).fit()\n",
    "t_p_i_model = smf.ols(\"total_payment_inv  ~ total_rec_prncp + total_payment\", df_prep.dataframe).fit()\n",
    "\n",
    "# Calculate Variance Inflation factor\n",
    "VIF_t_r_p_model = round(1/(1-t_r_p_model.rsquared), 2)\n",
    "VIF_t_p_model = round(1/(1-t_p_model.rsquared), 2)\n",
    "VIF_t_p_i_model = round(1/(1-t_p_i_model.rsquared), 2)\n",
    "\n",
    "#Print results\n",
    "print(f\"VIF scores: \\n total_rec_prncp: {VIF_t_r_p_model} \\n total_payment: {VIF_t_p_model} \\n total_payment_inv: {VIF_t_p_i_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIF values shows the presence of multicollinearity between total_rec_prncp, total_payment and total_payment_inv with the model predicting the total_payment_inv being the highest. However, total_payment and total_payment_inv are more helpful at intepreting the dataset than total_rec_prncp. Therefore that column will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep.remove_columns(\"total_rec_prncp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA_finance_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
